daten_mit_randsummen <- addmargins(daten)
daten_mit_randsummen
prop.table(daten_mit_randsummen, marin = 2)
daten_mit_randsummen <- addmargins(daten)
daten_mit_randsummen
prop.table(daten_mit_randsummen, margin = 2)
daten_mit_randsummen <- addmargins(daten)
daten_mit_randsummen
prop.table(daten_mit_randsummen, margin = 2,2)
h <- table(betriebe_daten)
h
n <- 3
for(i in 0:n){
betriebe_daten()
}
daten_rand <- addmargins(betriebe_daten)
daten_rand
n <- 3
for(i in 0:n){
betriebe_daten()
}
daten_rand <- addmargins(betriebe_daten)
daten_rand
zeilen <- nrow(betriebe_daten)
spalten <- ncol(betriebe_daten)
n <- sum(betriebe_daten)
daten_rand_erwartet <- daten_rand
for(i in 1:zeilen){
for(j in 1:spalten){
daten_rand_erwartet[i,j] <- daten_rand_erwartet[zeilen+1,j]*daten_rand_erwartet[i, spalten+1]/n
}
}
daten_rand_erwartet
#awContingency <- table(applewood$Location, applewood$Vehicle.Type)
#awContingency
```{r}
daten_mit_randsummen <- addmargins(daten)
daten_mit_randsummen
prop.table(daten_mit_randsummen, margin = 2)
daten_mit_randsummen <- addmargins(daten)
daten_mit_randsummen
rount(prop.table(daten_mit_randsummen, margin = 2),2)
daten_mit_randsummen <- addmargins(daten)
daten_mit_randsummen
rount(prop.table(daten_mit_randsummen, margin = 2);2)
daten_mit_randsummen <- addmargins(daten)
n <- sum(daten)
daten_mit_randsummen
rount()
daten_mit_randsummen <- addmargins(daten)
n <- sum(daten)
n
daten_mit_randsummen <- addmargins(daten)
daten_mit_randsummen
n <- sum(daten)
n
daten_relativ <- daten/n
daten_relativ
daten_relativ <- addmargin(daten/n)
daten_mit_randsummen <- addmargins(daten)
daten_mit_randsummen
n <- sum(daten)
n
daten_relativ <- addmargins(daten/n)
daten_relativ
daten_mit_randsummen <- addmargins(daten)
daten_mit_randsummen
n <- sum(daten)
n
daten_relativ <- round(addmargins(daten/n),2)
daten_relativ
daten_mit_randsummen <- addmargins(daten)
daten_mit_randsummen
n <- daten[1][3]
n
daten_mit_randsummen <- addmargins(daten)
daten_mit_randsummen
n <- daten[1,3]
daten_mit_randsummen <- addmargins(daten)
daten_mit_randsummen
n <- daten[3,1]
daten_mit_randsummen <- addmargins(daten)
daten_mit_randsummen
n <- daten_mit_randsummen[3,1]
n
knitr::opts_chunk$set(echo = TRUE)
addmargins(prop.table(ausschlag,1),1)
---
output:
html_document: default
html_notebook: default
pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Aufgabenblatt 9
## Statistik für Wirtschaftsinformatiker, Übung, HTW Berlin
### Michael Heimann, Shirin Riazy
Stand: `r format(Sys.Date(), format='%d.%m.%Y')`
## Wiederholung
* Was ist Korrelation?
Die Korrelation misst die Stärke des linearen Zusammenhang zweier Merkmale, nur für metrische Merkmale
* Was ist lineare Regression?
Einer Gerade die die Daren am besten beschreibt. Ma findet ein Optimum.
* Was ist die Gleichung für eine Gerade?
Y = ax+b
* Welche ist die unabhängige und welche ist die abhängige Variable in der Gleichung?
x ist unabhängig, y ist abhängig von x
## Aufgabe 9.1 (Regressionsrechnung)
Laden Sie den Datensatz `Readiq` aus dem R-Paket `BSDA`.
```{r, message=FALSE}
library(BSDA)
data(Readiq)
```
a) Machen Sie sich mit dem Datensatz vertraut. Was sind die beiden Merkmale in dem Datensatz?
```{r}
str(Readiq)
```
b) Fertigen Sie ein Streudiagramm (Scatterplot) an.
```{r}
plot(Readiq)
```
c) Bestimmem Sie den Korrelationskoeffizienten nach Pearson für die beiden Merkmale. Liegt keine, eine schwache, eine mittlere oder eine starke positive bzw. negative Korrelation zwischen beiden Merkmalen vor? Was bedeutet dies?
```{r}
r <- cor(Readiq$iq, Readiq$reading)
r
```
d) Bestimmen Sie die Geradengleichung der linearen Regression mit `Readiq$reading` als unabhängige und `Readiq$iq` als abhängige Variable. Berechnen Sie dazu die Werte von $a$ und $b$ in R mit den Formeln aus der Vorlesung. Hinweis: Obwohl `sd()` in R die Stichprobenvarianz und nicht die empirische Varianz aus den Formeln ist, kann `sd()` benutzt werden, da sich die Faktoren ($1/n$ bzw. $1/(n+1)$) in der Formel herauskürzen.
```{r}
a <- r*(sd(Readiq$iq)/sd(Readiq$reading))
a
b <- mean(Readiq$iq) - a*mean(Readiq$reading)
b
```
e) Fügen Sie dem Streuungsdiagramm von `Readiq$reading` und `Readiq$iq` die Regressionsgerade zu. Hinweis: Benutze `abline()`.
```{r}
plot(Readiq)
abline(b, a)
```
## Aufgabe 9.2 (Regressionsrechnung)
Über die Einführung des Mindestlohns wurde lange diskutiert. Ende 2007 veröffentlichte die *Wirtschaftswoche* folgende Daten zu Mindestlohn (in Euro) und
Arbeitslosenquote (in Prozent):
```{r}
library(knitr)
mindestlohn_tabelle <- data.frame(row.names = c("Irland", "Frankreich", "Großbritannien",
"Belgien", "Niederlande", "USA", "Spanien"),
Mindestlohn = c(8.65, 4.44, 8.2, 8.08, 8.08, 4.3, 3.42),
Arbeitslosenquote = c(4.4, 9.0, 5.5, 8.2, 5.5, 4.6, 8.5))
kable(mindestlohn_tabelle)
```
Gibt es einen linearen Zusammenhang zwischen *Mindestlohn* (unabhängige Variable) und
*Arbeitslosenquote*? Stellen Sie die Daten in einem  Streudiagramm dar, berechnen Sie die Regressionsgerade und fügen Sie sie dem Streudiagramm zu. Ist die Regressionsgerade eine sinnvolle Beschreibung der Daten?
```{r}
regression <- lm(mindestlohn_tabelle$Arbeitslosenquote ~ mindestlohn_tabelle$Mindestlohn)
plot(mindestlohn_tabelle$Arbeitslosenquote, mindestlohn_tabelle$Mindestlohn)
regression
abline(regression)
summary(regression)
```
Hinweis: Benutzen Sie diesmal die Funktion `lm()` in R zur Berechnung der Regressionsgeraden. Weisen Sie das Ergebnis des Aufrufs einer Variablen zu: `regression <- lm(...)`. Untersuchen Sie die Ausgabe von `regression` und `summary(regression)` und vergleichen Sie zur Interpretation auch die Folien der Vorlesung.
## Aufgabe 9.3 (Regressionsrechnung)
Benutzen Sie wieder die Daten des Mietspiegels in München von 2003.
```{r}
mietspiegel <- read.table("miete03.asc", header=TRUE)
```
a) Stellen Sie den Zusammenhang zwischen der Wohnfläche `mietspiegel$wfl` als unabhängiger und Nettomiete `mietspiegel$nm` als abhängiger Variable als Streudiagramm und durch die Regressionsgerade dar.
```{r}
regression <- lm(mietspiegel$nm ~ mietspiegel$wfl)
plot(mietspiegel$wfl, mietspiegel$nm)
regression
abline(regression)
```
b) Stellen Sie die Geradengleichung auf und berechnen die Nettomieten für die Wohnflächen 60qm, 100qm und 150qm, die sich aus der Geradengleichung als Schätzwerte ergeben.
```{r}
a <- regression$coefficients[2]
a
b <- regression$coefficients[1]
b
y <- a* c(60,100,150)+b
y
summary(regression)
```
c) Betrachten Sie wie in Aufgabe 9.2 das Regressionsobjekt mit `summary()`. Wie viel Prozent der Varianz von Nettomiete wird durch die Wohnfläche erklärt?
## Aufgabe 9.4 (Mehrdimensionale Regressionsrechnung)
Wir benutzen `mietspiegel` aus Aufgabe 9.3 und untersuchen, ob wir Nettomiete besser schätzen können, indem wir weitere Informationen berücksichtigen. Wir können z.B. das Merkmal `wohnbest` durch
```{r, eval=FALSE}
lm(nm ~ wfl + wohnbest, data = mietspiegel)
```
zusätzlich zu `wfl` als Information der Regression zufügen.
Die Qualität des Regressionsmodells kann in `summary()` anhand der Werte von
* *Residuals* (je kleiner die Beträge, desto besser)
* *Residual Standard Error* (je kleiner desto besser)
* *Adjusted R-Squared* (je größer desto besser)
abgelesen werden.
Fügen Sie `wohnbest`, `ww0` und `kueche` jeweils einzeln und in Kombination dem Regressionsmodell zu und vergleichen Sie die Qualität der Modelle mit Hilfe von `summary()`. Welches ist das beste Modell?
## Aufgabe 9.5 (Regressionsrechnung mit Polynomen höherer Ordnung)
Lineare Regression kann auch mit komplexeren Funktionen durchgeführt werden wie z.B. quadratischen Funktionen $f(x) = a_1 \cdot x + a_2 \cdot x^2 + b$. In R kann man für diesen Fall statt der Formel `y ~ x` in der Funktion `lm()` Formeln wie `y ~ poly(x,2)` für ein quadratisches Polynom benutzen.
Berechnen Sie für den folgenden Datensatz $(x,y)$ eine lineare Regression mit linearen, quadratischen und kubischen Formeln. Plotten Sie alle drei Ergebnisse als Kurven (Linienplot) im Streudiagramm der Daten. Vergleichen Sie die Güte der drei Modelle über `summary()`.
```{r}
x <- runif(100, 0, 100)
e <- runif(100, 0, 50)
y <- x^2 + x * e
```
## Aufgabe 9.6 (Rangkorrelation)
Berechnen Sie in folgenden drei Datensätzen die Pearson-, Kendall- und Spearman-Korrelationen (r, Rho, Tau) zwischen $x$ und $y$. Erklären Sie die Unterschiede in den Werten. Erstellen Sie zur Hilfe Streudiagramme der drei Datensätze. Welche Korrelationsmaße sind warum für welchen Datensatz geeignet oder nicht?
```{r}
y1 <- x + e
y2 <- y1^4
y3 <- (x - 50)^2 + 30 * e
data1 <- data.frame(x=x, y=y1)
data2 <- data.frame(x=x, y=y2)
data3 <- data.frame(x=x, y=y3)
```
knitr::opts_chunk$set(echo = TRUE)
r <- cor(Readiq$iq, Readiq$reading)
r
str(Readiq)
library(BSDA)
data(Readiq)
str(Readiq)
knitr::opts_chunk$set(echo = TRUE)
library(BSDA)
data(Readiq)
str(Readiq)
plot(Readiq)
r <- cor(Readiq$iq, Readiq$reading)
r
a <- r*(sd(Readiq$iq)/sd(Readiq$reading))
a
b <- mean(Readiq$iq) - a*mean(Readiq$reading)
b
plot(Readiq)
abline(b, a)
library(knitr)
mindestlohn_tabelle <- data.frame(row.names = c("Irland", "Frankreich", "Großbritannien",
"Belgien", "Niederlande", "USA", "Spanien"),
Mindestlohn = c(8.65, 4.44, 8.2, 8.08, 8.08, 4.3, 3.42),
Arbeitslosenquote = c(4.4, 9.0, 5.5, 8.2, 5.5, 4.6, 8.5))
kable(mindestlohn_tabelle)
kable(mindestlohn_tabelle)
mietspiegel <- read.table("miete03.asc", header=TRUE)
regression <- lm(mietspiegel$nm ~ mietspiegel$wfl)
plot(mietspiegel$wfl, mietspiegel$nm)
regression
abline(regression)
lm(nm ~ wfl + wohnbest, data = mietspiegel)
lm(nm ~ wfl + wohnbest + kueche , data = mietspiegel)
a <- regression$coefficients[2]
a
b <- regression$coefficients[1]
b
y <- a* c(60,100,150)+b
y
summary(regression)
lm(nm ~ wfl + wohnbest, data = mietspiegel)
regression <- lm(nm ~ wfl + wohnbest + kueche, data = mietspiegel)
plot(mietspiegel$wfl, mietspiegel$nm)
regression
abline(regression)
regression <- lm(nm ~ wfl + wohnbest + kueche, data = mietspiegel)
plot(mietspiegel$wfl, mietspiegel$nm)
regression
abline(regression)
regression <- lm(nm ~ wfl + wohnbest + kueche, data = mietspiegel)
plot(mietspiegel$wfl, mietspiegel$nm)
regression
abline(regression)
regression <- lm(nm ~ wfl, data = mietspiegel)
plot(mietspiegel$wfl, mietspiegel$nm)
regression
abline(regression)
regression_wfl <- lm(nm ~ wfl, data = mietspiegel)
plot(mietspiegel$wfl, mietspiegel$nm)
regression
abline(regression)
regression_wfl <- lm(nm ~ wfl, data = mietspiegel)
summary(regression_wfl)
regression_wfl <- lm(nm ~ wfl, data = mietspiegel)
summary(regression_wfl)
regression_wfl_wohnbest <- lm(nm ~ wfl + wohnbest, data = mietspiegel)
summary(regression_wfl_wohnbest)
regression_wfl <- lm(nm ~ wfl, data = mietspiegel)
summary(regression_wfl)
regression_wohnbest <- lm(nm ~ wohnbest, data = mietspiegel)
summary(regression_wohnbest)
regression_ww0 <- lm(nm ~ ww0, data = mietspiegel)
summary(regression_ww0)
regression_wfl_wohnbest <- lm(nm ~ wfl + wohnbest, data = mietspiegel)
summary(regression_wfl_wohnbest)
regression_kueche <- lm(nm ~ kueche, data = mietspiegel)
summary(regression_kueche)
regression_wfl <- lm(nm ~ wfl, data = mietspiegel)
summary(regression_wfl)
regression_wohnbest <- lm(nm ~ wohnbest, data = mietspiegel)
summary(regression_wohnbest)
regression_ww0 <- lm(nm ~ ww0, data = mietspiegel)
summary(regression_ww0)
regression_kueche <- lm(nm ~ kueche, data = mietspiegel)
summary(regression_kueche)
regression_wfl_wohnbest <- lm(nm ~ wfl + ww0 + wohnbest, data = mietspiegel)
summary(regression_wfl_wohnbest)
regression_all <- lm(nm ~ wfl + ww0 + wohnbest + kueche, data = mietspiegel)
summary(regression_all)
regression_wfl <- lm(nm ~ wfl, data = mietspiegel)
summary(regression_wfl)
regression_wohnbest <- lm(nm ~ wohnbest, data = mietspiegel)
summary(regression_wohnbest)
regression_ww0 <- lm(nm ~ ww0, data = mietspiegel)
summary(regression_ww0)
regression_kueche <- lm(nm ~ kueche, data = mietspiegel)
summary(regression_kueche)
regression_all <- lm(nm ~ wfl + ww0 + wohnbest + kueche, data = mietspiegel)
summary(regression_all)
regression_wfl_ww0 <- lm(nm ~ wfl + ww0, data = mietspiegel)
summary(regression_wfl_ww0)
regression_wfl_wohnbest <- lm(nm ~ wfl + wohnbest, data = mietspiegel)
summary(regression_wfl_wohnbest)
regression_wfl_kueche <- lm(nm ~ wfl + kueche, data = mietspiegel)
summary(regression_wfl_kueche)
regression_wfl_ww0_wohnbest <- lm(nm ~ wfl + ww0 + wohnbest, data = mietspiegel)
summary(regression_wfl_ww0_wohnbest)
regression_wfl_ww0_kueche <- lm(nm ~ wfl + ww0 + kueche, data = mietspiegel)
summary(regression_all)
regression_all <- lm(nm ~ wfl + ww0 + wohnbest + kueche, data = mietspiegel)
summary(regression_all)
regression_wfl_ww0_kueche <- lm(nm ~ wfl + ww0 + kueche, data = mietspiegel)
summary(regression_all)
regression_wfl_ww0_kueche <- lm(nm ~ wfl + ww0 + kueche, data = mietspiegel)
summary(regression_all)
summary(regression_wfl_ww0_kueche)
regression_all <- lm(nm ~ wfl + ww0 + wohnbest + kueche, data = mietspiegel)
summary(regression_all)
x <- runif(100, 0, 100)
e <- runif(100, 0, 50)
y <- x^2 + x * e
x <- runif(100, 0, 100)
e <- runif(100, 0, 50)
y <- x^2 + x * e
regression <- lm(y ~ poly(x,2))
lineplot(e, x)
x <- runif(100, 0, 100)
e <- runif(100, 0, 50)
y <- x^2 + x * e
regression <- lm(y ~ poly(x,2))
plot(e, x)
regression
abline(regression)
abline(regression)
x <- runif(100, 0, 100)
e <- runif(100, 0, 50)
y <- x^2 + x * e
regression <- lm(y ~ poly(x,2))
plot(x, y)
regression
abline(regression)
regression
x <- runif(100, 0, 100)
e <- runif(100, 0, 50)
y <- x^2 + x * e
regression <- lm(y ~ poly(x,2))
plot(x, y)
regression
linex(regression)
x <- runif(100, 0, 100)
e <- runif(100, 0, 50)
y <- x^2 + x * e
regression <- lm(y ~ poly(x,2))
plot(x, y)
regression
xp <- 0:100
yp <- predict(regression, data.frame(x=xp))
lines(xp, yp)
x <- runif(100, 0, 100)
e <- runif(100, 0, 50)
y <- x^2 + x * e
regression1 <- lm(y ~ x)
regression2 <- lm(y ~ poly(x,2))
regression3 <- lm(y ~ poly(x,3))
plot(x, y)
regression
xp <- 0:100
yp1 <- predict(regression1, data.frame(x=xp))
yp2 <- predict(regression2, data.frame(x=xp))
yp3 <- predict(regression3, data.frame(x=xp))
lines(xp, yp1, col="red")
lines(xp, yp2, col="blue")
lines(xp, yp3, col="green")
summary(regression1)
summary(regression2)
summary(regression3)
y1 <- x + e
y2 <- y1^4
y3 <- (x - 50)^2 + 30 * e
data1 <- data.frame(x=x, y=y1)
data2 <- data.frame(x=x, y=y2)
data3 <- data.frame(x=x, y=y3)
cor(data1, method = "pearson")
y1 <- x + e
y2 <- y1^4
y3 <- (x - 50)^2 + 30 * e
data1 <- data.frame(x=x, y=y1)
data2 <- data.frame(x=x, y=y2)
data3 <- data.frame(x=x, y=y3)
cor(data1, method = "pearson")
cor(data2, method = "pearson")
cor(data3, method = "pearson")
y1 <- x + e
y2 <- y1^4
y3 <- (x - 50)^2 + 30 * e
data1 <- data.frame(x=x, y=y1)
data2 <- data.frame(x=x, y=y2)
data3 <- data.frame(x=x, y=y3)
cor(data1, method = "pearson")
cor(data2, method = "pearson")
cor(data3, method = "pearson")
cor(data1, method = "kendall")
cor(data2, method = "kendall")
cor(data3, method = "kendall")
cor(data1, method = "spearman")
cor(data2, method = "spearman")
cor(data3, method = "spearman")
y1 <- x + e
y2 <- y1^4
y3 <- (x - 50)^2 + 30 * e
data1 <- data.frame(x=x, y=y1)
data2 <- data.frame(x=x, y=y2)
data3 <- data.frame(x=x, y=y3)
cor(data1, method = "pearson")
cor(data2, method = "pearson")
cor(data3, method = "pearson")
--
cor(data1, method = "kendall")
cor(data2, method = "kendall")
cor(data3, method = "kendall")
cor(data1, method = "spearman")
cor(data2, method = "spearman")
cor(data3, method = "spearman")
y1 <- x + e
y2 <- y1^4
y3 <- (x - 50)^2 + 30 * e
data1 <- data.frame(x=x, y=y1)
data2 <- data.frame(x=x, y=y2)
data3 <- data.frame(x=x, y=y3)
cor(data1, method = "pearson")
cor(data2, method = "pearson")
cor(data3, method = "pearson")
cor(data1, method = "kendall")
cor(data2, method = "kendall")
cor(data3, method = "kendall")
cor(data1, method = "spearman")
cor(data2, method = "spearman")
cor(data3, method = "spearman")
cor(x,y1, method = "pearson")
cor(x,y1, method = "pearson"); cor(x,y1, method = "kendall"); cor(x,y1, method = "spearman")
cor(x,y2, method = "pearson"); cor(x,y2, method = "kendall"); cor(x,y2, method = "spearman")
cor(x,y3, method = "pearson"); cor(x,y3, method = "kendall"); cor(x,y3, method = "spearman")
cor(x,y1, method = "pearson"); cor(x,y1, method = "kendall"); cor(x,y1, method = "spearman")
cor(x,y2, method = "pearson"); cor(x,y2, method = "kendall"); cor(x,y2, method = "spearman")
cor(x,y3, method = "pearson"); cor(x,y3, method = "kendall"); cor(x,y3, method = "spearman")
plot(data1)
cor(x,y1, method = "pearson"); cor(x,y1, method = "kendall"); cor(x,y1, method = "spearman")
plot(data1)
cor(x,y1, method = "pearson"); cor(x,y1, method = "kendall"); cor(x,y1, method = "spearman")
plot(data1)
y1 <- x + e
y2 <- y1^4
y3 <- (x - 50)^2 + 30 * e
data1 <- data.frame(x=x, y=y1)
data2 <- data.frame(x=x, y=y2)
data3 <- data.frame(x=x, y=y3)
cor(x,y1, method = "pearson"); cor(x,y1, method = "kendall"); cor(x,y1, method = "spearman")
plot(data1)
cor(x,y2, method = "pearson"); cor(x,y2, method = "kendall"); cor(x,y2, method = "spearman")
cor(x,y3, method = "pearson"); cor(x,y3, method = "kendall"); cor(x,y3, method = "spearman")
cor(data1, method = "pearson")
cor(data2, method = "pearson")
cor(data3, method = "pearson")
cor(data1, method = "kendall")
cor(data2, method = "kendall")
cor(data3, method = "kendall")
cor(data1, method = "spearman")
cor(data2, method = "spearman")
cor(data3, method = "spearman")
cor(x,y1, method = "pearson"); cor(x,y1, method = "kendall"); cor(x,y1, method = "spearman")
cor(x,y1, method = "pearson");  cor(x,y1, method = "spearman"); cor(x,y1, method = "kendall")
plot(data1)
cor(x,y1, method = "pearson");  cor(x,y1, method = "spearman"); cor(x,y1, method = "kendall")
plot(data2)
cor(x,y2, method = "pearson"); cor(x,y2, method = "spearman"); cor(x,y2, method = "kendall")
plot(data3)
cor(x,y3, method = "pearson"); cor(x,y3, method = "spearman"); cor(x,y3, method = "kendall")
##adjusted rsquared wird schlechter mit zunehmender komplexität, bzw schlechter von polynom 2. zu 3. ordnung
x <- runif(100, 0, 100)
e <- runif(100, 0, 50)
y <- x^2 + x * e
regression1 <- lm(y ~ x)
regression2 <- lm(y ~ poly(x,2))
regression3 <- lm(y ~ poly(x,3))
plot(x, y)
regression
xp <- 0:100
yp1 <- predict(regression1, data.frame(x=xp))
yp2 <- predict(regression2, data.frame(x=xp))
yp3 <- predict(regression3, data.frame(x=xp))
summary(regression1)
summary(regression2)
summary(regression3)
lines(xp, yp1, col="red")
lines(xp, yp2, col="blue")
lines(xp, yp3, col="green")
library(shiny); runApp('C:/Users/49177/git/htw/Statistik/StatistikSoSe2022Gruppe13/ShinyAppGruppe13.R')
